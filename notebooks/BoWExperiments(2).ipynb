{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tulane-cmps6730/project-legal.git"
      ],
      "metadata": {
        "id": "9BebeeGEwAaw",
        "outputId": "6870eed3-54f7-4177-8c1f-f27c457336b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project-legal'...\n",
            "remote: Enumerating objects: 967, done.\u001b[K\n",
            "remote: Counting objects: 100% (967/967), done.\u001b[K\n",
            "remote: Compressing objects: 100% (483/483), done.\u001b[K\n",
            "remote: Total 967 (delta 501), reused 920 (delta 469), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (967/967), 5.42 MiB | 6.18 MiB/s, done.\n",
            "Resolving deltas: 100% (501/501), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r project-legal/requirements.txt"
      ],
      "metadata": {
        "id": "z3Z7YOX4wI6y",
        "outputId": "8826ee39-eaaf-4805-d818-0e515bb8be0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==2.1.0 (from -r project-legal/requirements.txt (line 1))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp==3.9.3 (from -r project-legal/requirements.txt (line 2))\n",
            "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 4)) (1.4.4)\n",
            "Collecting asttokens==2.4.1 (from -r project-legal/requirements.txt (line 5))\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 7)) (4.0.3)\n",
            "Requirement already satisfied: attrs==23.2.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 8)) (23.2.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 9)) (4.12.3)\n",
            "Collecting blinker==1.7.0 (from -r project-legal/requirements.txt (line 10))\n",
            "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting bs4==0.0.2 (from -r project-legal/requirements.txt (line 11))\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: certifi==2024.2.2 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 12)) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 13)) (3.3.2)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 14)) (8.1.7)\n",
            "Collecting comm==0.2.2 (from -r project-legal/requirements.txt (line 15))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting contourpy==1.2.0 (from -r project-legal/requirements.txt (line 16))\n",
            "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cssselect==1.2.0 (from -r project-legal/requirements.txt (line 17))\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 18)) (0.12.1)\n",
            "Collecting datasets==2.18.0 (from -r project-legal/requirements.txt (line 19))\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting debugpy==1.8.1 (from -r project-legal/requirements.txt (line 20))\n",
            "  Downloading debugpy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator==5.1.1 (from -r project-legal/requirements.txt (line 21))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting dill==0.3.8 (from -r project-legal/requirements.txt (line 22))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 23)) (0.1.8)\n",
            "Collecting evaluate==0.4.1 (from -r project-legal/requirements.txt (line 24))\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exceptiongroup==1.2.0 (from -r project-legal/requirements.txt (line 25))\n",
            "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting executing==2.0.1 (from -r project-legal/requirements.txt (line 26))\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting fake-useragent==1.5.0 (from -r project-legal/requirements.txt (line 27))\n",
            "  Downloading fake_useragent-1.5.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: filelock==3.13.4 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 28)) (3.13.4)\n",
            "Collecting Flask==3.0.2 (from -r project-legal/requirements.txt (line 29))\n",
            "  Downloading flask-3.0.2-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.3/101.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Flask-WTF==1.2.1 (from -r project-legal/requirements.txt (line 30))\n",
            "  Downloading flask_wtf-1.2.1-py3-none-any.whl (12 kB)\n",
            "Collecting flatbuffers==24.3.7 (from -r project-legal/requirements.txt (line 31))\n",
            "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting fonttools==4.49.0 (from -r project-legal/requirements.txt (line 32))\n",
            "  Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: frozenlist==1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 33)) (1.4.1)\n",
            "Collecting fsspec==2024.3.1 (from -r project-legal/requirements.txt (line 34))\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast==0.5.4 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 35)) (0.5.4)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 36)) (0.2.0)\n",
            "Collecting grpcio==1.62.1 (from -r project-legal/requirements.txt (line 37))\n",
            "  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py==3.10.0 (from -r project-legal/requirements.txt (line 38))\n",
            "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: html5lib==1.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 39)) (1.1)\n",
            "Collecting huggingface-hub==0.21.4 (from -r project-legal/requirements.txt (line 40))\n",
            "  Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==3.6 (from -r project-legal/requirements.txt (line 41))\n",
            "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imbalanced-learn==0.12.2 (from -r project-legal/requirements.txt (line 42))\n",
            "  Downloading imbalanced_learn-0.12.2-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.0/258.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imblearn==0.0 (from -r project-legal/requirements.txt (line 43))\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting importlib_metadata==7.0.2 (from -r project-legal/requirements.txt (line 44))\n",
            "  Downloading importlib_metadata-7.0.2-py3-none-any.whl (24 kB)\n",
            "Collecting IProgress==0.4 (from -r project-legal/requirements.txt (line 45))\n",
            "  Downloading IProgress-0.4-py3-none-any.whl (11 kB)\n",
            "Collecting ipykernel==6.29.3 (from -r project-legal/requirements.txt (line 46))\n",
            "  Downloading ipykernel-6.29.3-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython==8.22.2 (from -r project-legal/requirements.txt (line 47))\n",
            "  Downloading ipython-8.22.2-py3-none-any.whl (811 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipywidgets==8.1.2 (from -r project-legal/requirements.txt (line 48))\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itsdangerous==2.1.2 (from -r project-legal/requirements.txt (line 49))\n",
            "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting jedi==0.19.1 (from -r project-legal/requirements.txt (line 50))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2==3.1.3 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 51)) (3.1.3)\n",
            "Collecting joblib==1.3.2 (from -r project-legal/requirements.txt (line 52))\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter_client==8.6.1 (from -r project-legal/requirements.txt (line 53))\n",
            "  Downloading jupyter_client-8.6.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 54)) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab_widgets==3.0.10 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 55)) (3.0.10)\n",
            "Collecting keras==3.0.5 (from -r project-legal/requirements.txt (line 56))\n",
            "  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 57)) (1.4.5)\n",
            "Collecting libclang==16.0.6 (from -r project-legal/requirements.txt (line 58))\n",
            "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lxml==5.1.0 (from -r project-legal/requirements.txt (line 59))\n",
            "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Markdown==3.6 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 60)) (3.6)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 61)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 62)) (2.1.5)\n",
            "Collecting matplotlib==3.8.3 (from -r project-legal/requirements.txt (line 63))\n",
            "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib-inline==0.1.6 (from -r project-legal/requirements.txt (line 64))\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 65)) (0.1.2)\n",
            "Collecting ml-dtypes==0.3.2 (from -r project-legal/requirements.txt (line 66))\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 67)) (1.3.0)\n",
            "Requirement already satisfied: multidict==6.0.5 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 68)) (6.0.5)\n",
            "Collecting multiprocess==0.70.16 (from -r project-legal/requirements.txt (line 69))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex==0.0.7 (from -r project-legal/requirements.txt (line 70))\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 71)) (1.6.0)\n",
            "Requirement already satisfied: networkx==3.3 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 72)) (3.3)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 73)) (3.8.1)\n",
            "Collecting numpy==1.26.4 (from -r project-legal/requirements.txt (line 74))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from -r project-legal/requirements.txt (line 75))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from -r project-legal/requirements.txt (line 76))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from -r project-legal/requirements.txt (line 77))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from -r project-legal/requirements.txt (line 78))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from -r project-legal/requirements.txt (line 79))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from -r project-legal/requirements.txt (line 80))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from -r project-legal/requirements.txt (line 81))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from -r project-legal/requirements.txt (line 82))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from -r project-legal/requirements.txt (line 83))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from -r project-legal/requirements.txt (line 84))\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from -r project-legal/requirements.txt (line 85))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from -r project-legal/requirements.txt (line 86))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 87)) (3.3.0)\n",
            "Requirement already satisfied: packaging==24.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 88)) (24.0)\n",
            "Collecting pandas==2.2.1 (from -r project-legal/requirements.txt (line 89))\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parse==1.20.1 (from -r project-legal/requirements.txt (line 90))\n",
            "  Downloading parse-1.20.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting parso==0.8.3 (from -r project-legal/requirements.txt (line 91))\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 92)) (4.9.0)\n",
            "Collecting pillow==10.2.0 (from -r project-legal/requirements.txt (line 93))\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs==4.2.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 94)) (4.2.0)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.43 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 95)) (3.0.43)\n",
            "Collecting protobuf==4.25.3 (from -r project-legal/requirements.txt (line 96))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil==5.9.8 (from -r project-legal/requirements.txt (line 97))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 98)) (0.7.0)\n",
            "Collecting pure-eval==0.2.2 (from -r project-legal/requirements.txt (line 99))\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting pyarrow==15.0.2 (from -r project-legal/requirements.txt (line 100))\n",
            "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix==0.6 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 101)) (0.6)\n",
            "Collecting pyee==11.1.0 (from -r project-legal/requirements.txt (line 102))\n",
            "  Downloading pyee-11.1.0-py3-none-any.whl (15 kB)\n",
            "Collecting Pygments==2.17.2 (from -r project-legal/requirements.txt (line 103))\n",
            "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 104)) (3.1.2)\n",
            "Collecting pyppeteer==2.0.0 (from -r project-legal/requirements.txt (line 105))\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyquery==2.0.0 (from -r project-legal/requirements.txt (line 106))\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r project-legal/requirements.txt (line 107))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz==2024.1 (from -r project-legal/requirements.txt (line 108))\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 109)) (6.0.1)\n",
            "Collecting pyzmq==25.1.2 (from -r project-legal/requirements.txt (line 110))\n",
            "  Downloading pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex==2023.12.25 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 111)) (2023.12.25)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 112)) (2.31.0)\n",
            "Collecting requests-html==0.10.0 (from -r project-legal/requirements.txt (line 113))\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Collecting responses==0.18.0 (from -r project-legal/requirements.txt (line 114))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: rich==13.7.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 115)) (13.7.1)\n",
            "Collecting safetensors==0.4.2 (from -r project-legal/requirements.txt (line 116))\n",
            "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.4.1.post1 (from -r project-legal/requirements.txt (line 117))\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.12.0 (from -r project-legal/requirements.txt (line 118))\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 119)) (1.16.0)\n",
            "Requirement already satisfied: soupsieve==2.5 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 120)) (2.5)\n",
            "Collecting stack-data==0.6.3 (from -r project-legal/requirements.txt (line 121))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: sympy==1.12 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 122)) (1.12)\n",
            "Collecting tensorboard==2.16.2 (from -r project-legal/requirements.txt (line 123))\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-data-server==0.7.2 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 124)) (0.7.2)\n",
            "Collecting tensorflow==2.16.1 (from -r project-legal/requirements.txt (line 125))\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m967.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.36.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 126)) (0.36.0)\n",
            "Requirement already satisfied: termcolor==2.4.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 127)) (2.4.0)\n",
            "Collecting threadpoolctl==3.3.0 (from -r project-legal/requirements.txt (line 128))\n",
            "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
            "Collecting tokenizers==0.15.2 (from -r project-legal/requirements.txt (line 129))\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.2.2 (from -r project-legal/requirements.txt (line 130))\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m774.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchinfo==1.8.0 (from -r project-legal/requirements.txt (line 131))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Collecting tornado==6.4 (from -r project-legal/requirements.txt (line 132))\n",
            "  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 133)) (4.66.2)\n",
            "Collecting traitlets==5.14.2 (from -r project-legal/requirements.txt (line 134))\n",
            "  Downloading traitlets-5.14.2-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.38.2 (from -r project-legal/requirements.txt (line 135))\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 136)) (2.2.0)\n",
            "Requirement already satisfied: typing_extensions==4.11.0 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 137)) (4.11.0)\n",
            "Requirement already satisfied: tzdata==2024.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 138)) (2024.1)\n",
            "Collecting urllib3==1.26.18 (from -r project-legal/requirements.txt (line 139))\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting w3lib==2.1.2 (from -r project-legal/requirements.txt (line 140))\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 141)) (0.2.13)\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 142)) (0.5.1)\n",
            "Collecting websockets==10.4 (from -r project-legal/requirements.txt (line 143))\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Werkzeug==3.0.1 (from -r project-legal/requirements.txt (line 144))\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting widgetsnbextension==4.0.10 (from -r project-legal/requirements.txt (line 145))\n",
            "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt==1.16.0 (from -r project-legal/requirements.txt (line 146))\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting WTForms==3.1.2 (from -r project-legal/requirements.txt (line 147))\n",
            "  Downloading wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash==3.4.1 (from -r project-legal/requirements.txt (line 148))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yarl==1.9.4 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 149)) (1.9.4)\n",
            "Requirement already satisfied: zipp==3.18.1 in /usr/local/lib/python3.10/dist-packages (from -r project-legal/requirements.txt (line 150)) (3.18.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse==1.6.3->-r project-legal/requirements.txt (line 6)) (0.43.0)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0->-r project-legal/requirements.txt (line 19)) (2023.6.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.16.2->-r project-legal/requirements.txt (line 123)) (67.7.2)\n",
            "INFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets==2.18.0->-r project-legal/requirements.txt (line 19))\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.12.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.0/154.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "\u001b[31mERROR: Cannot install -r project-legal/requirements.txt (line 130), -r project-legal/requirements.txt (line 40), fsspec==2024.3.1, fsspec[http]==2023.1.0, fsspec[http]==2023.10.0, fsspec[http]==2023.12.0, fsspec[http]==2023.12.1, fsspec[http]==2023.12.2, fsspec[http]==2023.3.0, fsspec[http]==2023.4.0, fsspec[http]==2023.5.0, fsspec[http]==2023.6.0, fsspec[http]==2023.9.0, fsspec[http]==2023.9.1, fsspec[http]==2023.9.2 and fsspec[http]==2024.2.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.6.0 depends on fsspec 2023.6.0 (Installed)\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2024.2.0 depends on fsspec 2024.2.0 (from https://files.pythonhosted.org/packages/ad/30/2281c062222dc39328843bd1ddd30ff3005ef8e30b2fd09c4d2792766061/fsspec-2024.2.0-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.12.2 depends on fsspec 2023.12.2 (from https://files.pythonhosted.org/packages/70/25/fab23259a52ece5670dcb8452e1af34b89e6135ecc17cd4b54b4b479eac6/fsspec-2023.12.2-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.12.1 depends on fsspec 2023.12.1 (from https://files.pythonhosted.org/packages/96/0e/2be9b5a2e3f736577e749bbdf27a1e7e965041e1c908d49dedf56eeb2b8a/fsspec-2023.12.1-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.12.0 depends on fsspec 2023.12.0 (from https://files.pythonhosted.org/packages/67/32/9276db0647d8142da3d9ec1af536522081813005a9d7aaebbdba082967c1/fsspec-2023.12.0-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.10.0 depends on fsspec 2023.10.0 (from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.9.2 depends on fsspec 2023.9.2 (from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.9.1 depends on fsspec 2023.9.1 (from https://files.pythonhosted.org/packages/6a/af/c673e8c663e17bd4fb201a6f029153ad5d7023aa4442d81c7987743db379/fsspec-2023.9.1-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.9.0 depends on fsspec 2023.9.0 (from https://files.pythonhosted.org/packages/3a/9f/b40e8e5be886143379000af5fc0c675352d59e82fd869d24bf784161dc77/fsspec-2023.9.0-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.5.0 depends on fsspec 2023.5.0 (from https://files.pythonhosted.org/packages/ec/4e/397b234a369df06ec782666fcdf9791d125ca6de48729814b381af8c6c03/fsspec-2023.5.0-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.4.0 depends on fsspec 2023.4.0 (from https://files.pythonhosted.org/packages/d6/30/db3078afe553e9a07c87534cbfb87a8c8ebb083fa0a8847ca5bdc86b51a7/fsspec-2023.4.0-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.3.0 depends on fsspec 2023.3.0 (from https://files.pythonhosted.org/packages/4f/65/887925f1549fcb6ac3abb23a747c10f5ab083e8471fe568768b18bdb15b2/fsspec-2023.3.0-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.8))\n",
            "    The user requested fsspec==2024.3.1\n",
            "    huggingface-hub 0.21.4 depends on fsspec>=2023.5.0\n",
            "    torch 2.2.2 depends on fsspec\n",
            "    fsspec[http] 2023.1.0 depends on fsspec 2023.1.0 (from https://files.pythonhosted.org/packages/bd/64/f0d369ede0ca54fdd520bdee5086dbaf0af81dac53a2ce847bd1ec6e0bf1/fsspec-2023.1.0-py3-none-any.whl (from https://pypi.org/simple/fsspec/) (requires-python:>=3.7))\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Z79rhg4DwQiR",
        "outputId": "250b0fdb-cfe9-4ba1-f8b4-ee55fa93c12c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/542.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9giwlx6v-J9"
      },
      "source": [
        "## Experiments\n",
        "\n",
        "Let's start playing around with our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S7YZ2DIZv-J-"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset,Dataset,DatasetDict\n",
        "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification,AutoModel,AutoConfig\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJKxE3aWv-J_"
      },
      "source": [
        "# Read in training data\n",
        "This dataset contains 100 annotated terms of service contracts, each row represents a sentence, which carries on it a label. The label corresponds to a different type of potential unfairness, as defined by the authors of CLAUDETTE, the previous paper from which this dataset came from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P1Ul11COv-J_",
        "outputId": "fcefa89f-65b4-405c-b801-91e065b828ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  A  CH  CR  J  LAW  LTD  PINC  TER  USE document  document_ID  \\\n",
              "0           0  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
              "1           1  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
              "2           2  0   0   0  0    0    0     0    0    1  Mozilla            0   \n",
              "3           3  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
              "4           4  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
              "\n",
              "   label                                               text TER_targets  \\\n",
              "0      0             websites & communications terms of use         NaN   \n",
              "1      0  please read the terms of this entire document ...         NaN   \n",
              "2      1  by accessing or signing up to receive communic...         NaN   \n",
              "3      0  our websites include multiple domains such as ...         NaN   \n",
              "4      0  you may also recognize our websites by nicknam...         NaN   \n",
              "\n",
              "  LTD_targets A_targets CH_targets CR_targets  \n",
              "0         NaN       NaN        NaN        NaN  \n",
              "1         NaN       NaN        NaN        NaN  \n",
              "2         NaN       NaN        NaN        NaN  \n",
              "3         NaN       NaN        NaN        NaN  \n",
              "4         NaN       NaN        NaN        NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b9921ba-7311-412e-a327-f9e3354350a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>A</th>\n",
              "      <th>CH</th>\n",
              "      <th>CR</th>\n",
              "      <th>J</th>\n",
              "      <th>LAW</th>\n",
              "      <th>LTD</th>\n",
              "      <th>PINC</th>\n",
              "      <th>TER</th>\n",
              "      <th>USE</th>\n",
              "      <th>document</th>\n",
              "      <th>document_ID</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>TER_targets</th>\n",
              "      <th>LTD_targets</th>\n",
              "      <th>A_targets</th>\n",
              "      <th>CH_targets</th>\n",
              "      <th>CR_targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Mozilla</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>websites &amp; communications terms of use</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Mozilla</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>please read the terms of this entire document ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Mozilla</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>by accessing or signing up to receive communic...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Mozilla</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>our websites include multiple domains such as ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Mozilla</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>you may also recognize our websites by nicknam...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b9921ba-7311-412e-a327-f9e3354350a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b9921ba-7311-412e-a327-f9e3354350a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b9921ba-7311-412e-a327-f9e3354350a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e7088c49-dda9-446d-8d14-ca0e6b1354b0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7088c49-dda9-446d-8d14-ca0e6b1354b0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e7088c49-dda9-446d-8d14-ca0e6b1354b0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20417,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5894,\n        \"min\": 0,\n        \"max\": 20416,\n        \"num_unique_values\": 20417,\n        \"samples\": [\n          18006,\n          16987,\n          6586\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"J\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LAW\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LTD\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PINC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"USE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Oculus\",\n          \"Snap\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19915,\n        \"samples\": [\n          \"2.12 if delivery or collection is delayed through your unreasonable refusal to accept delivery or if you do not -lrb- within two weeks of our first attempt to deliver the product to you -rrb- accept delivery or collect the product from the carrier , then we may -lrb- without affecting any other right or remedy available to us -rrb- do either or both of the following :\",\n          \"onavo reserves the right to modify the terms and conditions of this agreement or its policies relating to the services at any time .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TER_targets\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 71,\n        \"samples\": [\n          \"[3]\",\n          \"[1, 0, 19, 25]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LTD_targets\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 206,\n        \"samples\": [\n          \"[5, 9, 3]\",\n          \"[5, 3, 7, 1, 2, 9, 14]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A_targets\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"[1, 0]\",\n          \"[1, 4, 5]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CH_targets\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"[0, 6]\",\n          \"[3, 2]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CR_targets\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"[4, 5]\",\n          \"[4, 0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = pd.read_csv('project-legal/data/dataset.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxC2vXygv-J_"
      },
      "source": [
        "Before we were trying binary classification, which wasn't producing great results, let's see if we can get better results using the individual types of labels as classified by the text. The logic here is that because each type of potential unfairness likely has some semantic differences, conglomerating them all into one made it difficult to pick them all out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuSBZoMCv-J_"
      },
      "source": [
        "# Multi-Label Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N1iEEijv-J_",
        "outputId": "9385504d-19cb-4d54-d784-fce98c8606ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "labels\n",
              "0    0.893324\n",
              "6    0.029681\n",
              "2    0.016849\n",
              "8    0.015085\n",
              "9    0.011853\n",
              "3    0.010286\n",
              "4    0.006661\n",
              "5    0.006122\n",
              "1    0.005192\n",
              "7    0.004947\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['labels'] = df.apply(lambda row: (1 if row['A'] == 1 else 2 if row['CH'] == 1 else 3 if row['CR'] == 1 else 4 if row['J'] == 1 else 5 if row['LAW'] == 1 else 6 if row['LTD'] == 1 else 7 if row['PINC'] == 1 else 8 if row['TER'] == 1 else 9 if row['USE'] == 1 else 0),axis=1)\n",
        "x_multi = df['text']\n",
        "y_multi = df['labels']\n",
        "df['labels'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wd6lGLFv-J_",
        "outputId": "b122954c-efac-4eae-d903-612df0a4b1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'FAIR',\n",
              " 1: 'A',\n",
              " 2: 'CH',\n",
              " 3: 'CR',\n",
              " 4: 'J',\n",
              " 5: 'LAW',\n",
              " 6: 'LTD',\n",
              " 7: 'PINC',\n",
              " 8: 'TER',\n",
              " 9: 'USE'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label2id = {\n",
        "    'FAIR':0,\n",
        "    'A':1,\n",
        "    'CH':2,\n",
        "    'CR':3,\n",
        "    'J':4,\n",
        "    'LAW':5,\n",
        "    'LTD':6,\n",
        "    'PINC':7,\n",
        "    'TER':8,\n",
        "    'USE':9\n",
        "}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPCRjO8sv-KA"
      },
      "source": [
        "# Binary Label Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbJz_jZnv-KA"
      },
      "outputs": [],
      "source": [
        "x, y = df['text'], df['label']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
        "rus = RandomUnderSampler(sampling_strategy=.8)\n",
        "x_train_res, y_train_res = rus.fit_resample(pd.DataFrame(x_train), pd.DataFrame(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAioZ8Wnv-KA",
        "outputId": "687f3212-e0bf-400b-d6b0-e203c1f1ba1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "0        2162\n",
              "1        1730\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train_res.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztyRyOW7v-KB"
      },
      "outputs": [],
      "source": [
        "train_df = pd.merge(x_train_res,y_train_res,left_index=True,right_index=True)\n",
        "test_df = pd.merge(x_test,y_test,left_index=True,right_index=True)\n",
        "val_df = pd.merge(x_val,y_val,left_index=True,right_index=True)\n",
        "train_df = train_df.sample(frac=1)\n",
        "subset_df = train_df.sample(50)\n",
        "test_subset_df = test_df.sample(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EUPdJ-_v-KB"
      },
      "outputs": [],
      "source": [
        "dataset_dict = DatasetDict(\n",
        "    {\n",
        "    \"train\":Dataset.from_pandas(train_df),\n",
        "    \"test\":Dataset.from_pandas(test_df),\n",
        "    \"val\":Dataset.from_pandas(val_df),\n",
        "    \"train_subset\":Dataset.from_pandas(subset_df),\n",
        "    \"test_subset\":Dataset.from_pandas(test_subset_df)\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnvntx6dv-KB"
      },
      "source": [
        "Split dataset into train, test and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmOV50tVv-KB"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], max_length=512,padding='max_length')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv42j1plv-KB",
        "outputId": "48588c39-9ef9-4db7-d61e-b0ed88c234b9",
        "colab": {
          "referenced_widgets": [
            "eaeb7dd9ab714121a8a6f61d48daac1d",
            "2ef300a479ef4657a423c8ff56976512",
            "6ec87f5b7b3f4ad0acc451dbc7ae77f3",
            "9e783b953c9b449dbf8c68d3f2f88152",
            "68f784530e05410aaa3d57ce84fd853c"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaeb7dd9ab714121a8a6f61d48daac1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3892 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ef300a479ef4657a423c8ff56976512",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2042 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec87f5b7b3f4ad0acc451dbc7ae77f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2042 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e783b953c9b449dbf8c68d3f2f88152",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68f784530e05410aaa3d57ce84fd853c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dict = dataset_dict.map(tokenize, batched=True, batch_size=len(dataset_dict))\n",
        "tokenized_dict.set_format('torch', columns=['input_ids', 'attention_mask',\"token_type_ids\",'label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cBFinkFv-KB"
      },
      "source": [
        "Now let's construct a custom classifier classifier to classify sentences as potentially unfair or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8eTS3v5v-KB"
      },
      "outputs": [],
      "source": [
        "class Classifier_v1(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(Classifier_v1,self).__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name,config = AutoConfig.from_pretrained(model_name,\n",
        "                                                                                              output_attention = True,\n",
        "                                                                                              output_hidden_state = True))\n",
        "        # New Layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.hidden = nn.Linear(self.model.config.hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input = None, attention_mask = None, token_type_ids = None,input_ids = None, labels = None):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
        "        pooler_output = outputs.pooler_output[0]\n",
        "        output = self.hidden(pooler_output)\n",
        "        logit = self.sigmoid(output)\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkX0g3e4v-KC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_dict['train'], shuffle=True, batch_size=1, collate_fn=data_collator\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_dict['test'], shuffle=True, batch_size=1,collate_fn=data_collator\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    tokenized_dict['val'], shuffle=True, batch_size=1,collate_fn=data_collator\n",
        ")\n",
        "train_subset_dataloader = DataLoader(\n",
        "    tokenized_dict['train_subset'], shuffle=True, batch_size=1,collate_fn=data_collator\n",
        ")\n",
        "test_subset_dataloader = DataLoader(\n",
        "    tokenized_dict['test_subset'], shuffle=True, batch_size=1,collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL3sfj3zv-KC"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gB_12Vpv-KC"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfPeilfav-KC"
      },
      "source": [
        "# Full Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZRKzptZv-KC"
      },
      "outputs": [],
      "source": [
        "def train(model,train_dataloader,test_dataloader,epochs,loss_fn,optimizer,num_train_samples,num_test_samples,lr_scheduler=None,testing=False):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_labels = []\n",
        "        train_preds = []\n",
        "        train_loss = 0\n",
        "        for batch in tqdm.tqdm(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            token_type_ids = batch['token_type_ids'].to(device)\n",
        "            label = batch['labels'].to(torch.float32).to(device)\n",
        "            output = model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
        "            loss = loss_fn(output[0],label)\n",
        "            train_loss += loss.item()\n",
        "            pred = torch.round(output[0])\n",
        "            train_preds.append(pred.detach().cpu().numpy())\n",
        "            train_labels.append(label.detach().cpu().numpy())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if lr_scheduler:\n",
        "                lr_scheduler.step(loss)\n",
        "        print(f\"Train Epoch: {epoch + 1} | Accuracy: {accuracy_score(train_labels,train_preds)} | Precision: {precision_score(train_labels,train_preds)} | Recall: {recall_score(train_labels,train_preds)} | F1: {f1_score(train_labels,train_preds)} | Loss: {train_loss/num_train_samples}\")\n",
        "        if testing:\n",
        "            preds = []\n",
        "            labels = []\n",
        "            test_loss = 0\n",
        "            for i,batch in tqdm.tqdm(enumerate(test_dataloader)):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                token_type_ids = batch['token_type_ids'].to(device)\n",
        "                label = batch['labels'].to(torch.float32).to(device)\n",
        "                with torch.no_grad():\n",
        "                    output = model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
        "                loss = loss_fn(output[0],label)\n",
        "                test_loss += (loss.item())\n",
        "                pred = torch.round(output[0])\n",
        "                preds.append(pred.detach().cpu().numpy())\n",
        "                labels.append(batch[\"labels\"].detach().cpu().numpy())\n",
        "            print(f\"Test Epoch: {epoch + 1} | Accuracy: {accuracy_score(labels,preds)} | Precision: {precision_score(labels,preds)} | Recall: {recall_score(labels,preds)} | F1: {f1_score(labels,preds)} | Loss: {test_loss/num_test_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sce2WBo1v-KC"
      },
      "outputs": [],
      "source": [
        "def test(model,test_dataloader,loss_fn):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    labels = []\n",
        "    for i,batch in tqdm.tqdm(enumerate(test_dataloader)):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "        label = batch['labels'].to(torch.float32).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
        "        loss = loss_fn(output,label)\n",
        "        pred = torch.round(output)\n",
        "        preds.append(pred.detach().cpu().numpy()[0])\n",
        "        labels.append(batch[\"labels\"].detach().cpu().numpy())\n",
        "    print(f\"Accuracy: {accuracy_score(labels,preds)} | Precision: {precision_score(labels,preds)} | Recall: {recall_score(labels,preds)} | F1: {f1_score(labels,preds)} | Loss: {loss}\")\n",
        "    return preds,labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAWygllbv-KC"
      },
      "source": [
        "Looking much better than before, but there is still a lot of room for improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BgB2rhRv-KC"
      },
      "source": [
        "This is not working, even with oversampling we are still not getting good results. I think the problem is that the pooler output, which is the (mean?) of all the 12 hidden states of BERT that we are using as input to our additional classifier is not capturing the information we need to understand the fairness of a sentence. Instead of using the pooler output, let's use a concatenation of all the hidden states of the BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IWoZ7Ziv-KC"
      },
      "outputs": [],
      "source": [
        "class Classifier_V2(nn.Module):\n",
        "    '''\n",
        "    This model is similar to the first one but instead of using the pooler output, it uses the hidden states of the model\n",
        "    The 'hidden_states_used' parameter is used to determine how many hidden states to use, smaller values of this will be less computationally expensive, but likely less accurate\n",
        "    '''\n",
        "    def __init__(self, model_name ,num_labels,hidden_states_used):\n",
        "        super(Classifier_V2,self).__init__()\n",
        "        self.hidden_states_used = hidden_states_used\n",
        "        self.model = BertModel.from_pretrained(model_name,config = BertConfig.from_pretrained(model_name,output_hidden_states = True,num_labels=num_labels))\n",
        "        self.hidden1 = nn.Linear(self.model.config.hidden_size*self.model.config.max_position_embeddings*self.hidden_states_used, 64)\n",
        "        self.hidden_p = nn.Linear(self.model.config.hidden_size, 64)\n",
        "        self.fc = nn.Linear(64, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        if num_labels == 1:\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            self.activation = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, attention_mask = None, token_type_ids = None,input_ids = None):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
        "        hidden_states = torch.cat(outputs.hidden_states[-self.hidden_states_used:],dim=0).view(1,-1)\n",
        "        pooler_output = outputs.pooler_output\n",
        "        x_pooler = self.hidden_p(self.dropout(pooler_output))\n",
        "        x_hidden = self.hidden1(self.dropout(hidden_states))\n",
        "        x = torch.add(x_pooler,x_hidden)\n",
        "        output = self.fc(x)\n",
        "        logit = self.activation(output)\n",
        "        return logit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP02FjK0v-KD"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "model_v2 = Classifier_V2('nlpaueb/legal-bert-base-uncased',1,6).to(device)\n",
        "optimizer = Adam(model_v2.parameters(),lr =.0000001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=2,patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNdpquLSv-KD",
        "outputId": "5782c873-2b98-4904-f454-14b6956c12ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:11<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Accuracy: 0.42 | Precision: 0.35294117647058826 | Recall: 0.25 | F1: 0.2926829268292683 | Loss: 0.7563589495420456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:12,  3.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 1 | Accuracy: 0.86 | Precision: 0.0 | Recall: 0.0 | F1: 0.0 | Loss: 0.5644528120756149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:20<00:00,  1.61s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 2 | Accuracy: 0.62 | Precision: 0.8571428571428571 | Recall: 0.25 | F1: 0.3870967741935484 | Loss: 0.6564678603410721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:12,  4.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 2 | Accuracy: 0.66 | Precision: 0.125 | Recall: 0.4 | F1: 0.19047619047619047 | Loss: 0.6509701585769654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:18<00:00,  1.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 3 | Accuracy: 0.74 | Precision: 0.6666666666666666 | Recall: 0.9166666666666666 | F1: 0.7719298245614035 | Loss: 0.6002006658911705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:12,  3.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 3 | Accuracy: 0.68 | Precision: 0.13333333333333333 | Recall: 0.4 | F1: 0.2 | Loss: 0.6116728174686432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:19<00:00,  1.60s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 4 | Accuracy: 0.82 | Precision: 0.8571428571428571 | Recall: 0.75 | F1: 0.8 | Loss: 0.5769010004401207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:12,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 4 | Accuracy: 0.86 | Precision: 0.25 | Recall: 0.2 | F1: 0.2222222222222222 | Loss: 0.5089388361573219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:22<00:00,  1.65s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 5 | Accuracy: 0.76 | Precision: 0.875 | Recall: 0.5833333333333334 | F1: 0.7 | Loss: 0.5542890709638596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:13,  3.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 5 | Accuracy: 0.76 | Precision: 0.1111111111111111 | Recall: 0.2 | F1: 0.14285714285714285 | Loss: 0.5196777975559235\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:21<00:00,  1.63s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 6 | Accuracy: 0.92 | Precision: 0.9166666666666666 | Recall: 0.9166666666666666 | F1: 0.9166666666666666 | Loss: 0.4833053112030029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:13,  3.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 6 | Accuracy: 0.72 | Precision: 0.23529411764705882 | Recall: 0.8 | F1: 0.36363636363636365 | Loss: 0.5877727711200714\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:25<00:00,  1.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 7 | Accuracy: 0.92 | Precision: 0.9545454545454546 | Recall: 0.875 | F1: 0.9130434782608695 | Loss: 0.44663905635476114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:13,  3.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 7 | Accuracy: 0.68 | Precision: 0.13333333333333333 | Recall: 0.4 | F1: 0.2 | Loss: 0.5622653564810753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:22<00:00,  1.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 8 | Accuracy: 0.92 | Precision: 1.0 | Recall: 0.8333333333333334 | F1: 0.9090909090909091 | Loss: 0.42834208011627195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:13,  3.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 8 | Accuracy: 0.72 | Precision: 0.2 | Recall: 0.6 | F1: 0.3 | Loss: 0.5754399782419205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:23<00:00,  1.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 9 | Accuracy: 0.94 | Precision: 0.9565217391304348 | Recall: 0.9166666666666666 | F1: 0.9361702127659575 | Loss: 0.41556101858615874\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:12,  3.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 9 | Accuracy: 0.58 | Precision: 0.13636363636363635 | Recall: 0.6 | F1: 0.2222222222222222 | Loss: 0.6365721142292022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:23<00:00,  1.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 10 | Accuracy: 0.9 | Precision: 1.0 | Recall: 0.7916666666666666 | F1: 0.8837209302325582 | Loss: 0.38957851752638817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:13,  3.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Epoch: 10 | Accuracy: 0.76 | Precision: 0.18181818181818182 | Recall: 0.4 | F1: 0.25 | Loss: 0.5084145966172219\n"
          ]
        }
      ],
      "source": [
        "train(model_v2,train_subset_dataloader,test_subset_dataloader,10,loss_fn,optimizer,num_train_samples=50,num_test_samples=50,testing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVHuzTqRv-KD"
      },
      "outputs": [],
      "source": [
        "del loss_fn\n",
        "del optimizer\n",
        "del model_v2\n",
        "del scheduler\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words Model"
      ],
      "metadata": {
        "id": "lp8bfw7lwahG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "gqNLo2ceyQrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts = df['text'].values\n",
        "labels = df['label'].values\n",
        "\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    # tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # removing stopwords to focus on meaningful vocab\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "texts_train = [preprocess(text) for text in texts_train]\n",
        "texts_test = [preprocess(text) for text in texts_test]"
      ],
      "metadata": {
        "id": "mBGM_m7cyPaI",
        "outputId": "0bb92a8e-e142-4279-fc7d-a607b004df0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "srpUz6KN1VaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.util import ngrams\n",
        "\n",
        "for column in df.columns:\n",
        "    df[column] = df[column].fillna('')\n",
        "    df[column] = df[column].astype(str)\n",
        "\n",
        "texts = df['text'].astype(str).values\n",
        "labels = df['label'].values\n",
        "\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# using porterstemmer reduce words to their roots\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(text, n=2):\n",
        "    # text -> tokens\n",
        "    tokens = word_tokenize(text)\n",
        "    # stem and removing stopwords\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
        "    # n-gram generation\n",
        "    n_grams = list(ngrams(tokens, n))\n",
        "    # flattening list of n-grams\n",
        "    n_grams = ['_'.join(gram) for gram in n_grams]\n",
        "    return tokens + n_grams\n",
        "\n",
        "# include n-grams in data processing\n",
        "texts_train = [preprocess(text, n=2) for text in texts_train]  # Use n=2 for bigrams, change as needed\n",
        "texts_test = [preprocess(text, n=2) for text in texts_test]"
      ],
      "metadata": {
        "id": "g3ZdWyni1VD-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW Model"
      ],
      "metadata": {
        "id": "p_ZHVBfRyXFR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r3wMFapmv-KD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# init CountVectorizer with 1-2 n-grams (should suffice for our purposes)\n",
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b')\n",
        "\n",
        "# preparing train data by joining tokens into strings (countvectorizor takes in strings)\n",
        "texts_train_joined = [' '.join(text) for text in texts_train]\n",
        "texts_test_joined = [' '.join(text) for text in texts_test]\n",
        "\n",
        "# fitting vectorizer on the train data\n",
        "X_train = vectorizer.fit_transform(texts_train_joined)\n",
        "\n",
        "# transforming test data based on fitted vocab\n",
        "X_test = vectorizer.transform(texts_test_joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classifier**"
      ],
      "metadata": {
        "id": "dvCWdm13yl-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# init/train classifier\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, labels_train)\n",
        "\n",
        "# predicting on test set\n",
        "labels_pred = classifier.predict(X_test)\n",
        "\n",
        "# getting accuracy\n",
        "accuracy = accuracy_score(labels_test, labels_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "h9oi60MMylQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f388e21-53d0-4e92-c5d6-d9e1f359754f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9419686581782566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Report**"
      ],
      "metadata": {
        "id": "LoHNsarEy1CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# performance analysis\n",
        "print(classification_report(labels_test, labels_pred))"
      ],
      "metadata": {
        "id": "q_CCNBS2y1uO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30f7b02-b9e1-4348-811a-d16824d653af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97      3632\n",
            "           1       0.84      0.59      0.69       452\n",
            "\n",
            "    accuracy                           0.94      4084\n",
            "   macro avg       0.89      0.79      0.83      4084\n",
            "weighted avg       0.94      0.94      0.94      4084\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zU7fEnp83Mvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}