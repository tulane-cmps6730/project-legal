{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Let's start playing around with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 00:25:01.561570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-27 00:25:03.151595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification,AutoModel,AutoConfig\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in training data\n",
    "This dataset contains 100 annotated terms of service contracts, each row represents a sentence, which carries on it a label. The label corresponds to a different type of potential unfairness, as defined by the authors of CLAUDETTE, the previous paper from which this dataset came from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A</th>\n",
       "      <th>CH</th>\n",
       "      <th>CR</th>\n",
       "      <th>J</th>\n",
       "      <th>LAW</th>\n",
       "      <th>LTD</th>\n",
       "      <th>PINC</th>\n",
       "      <th>TER</th>\n",
       "      <th>USE</th>\n",
       "      <th>document</th>\n",
       "      <th>document_ID</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>TER_targets</th>\n",
       "      <th>LTD_targets</th>\n",
       "      <th>A_targets</th>\n",
       "      <th>CH_targets</th>\n",
       "      <th>CR_targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>websites &amp; communications terms of use</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>please read the terms of this entire document ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>by accessing or signing up to receive communic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>our websites include multiple domains such as ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you may also recognize our websites by nicknam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  A  CH  CR  J  LAW  LTD  PINC  TER  USE document  document_ID  \\\n",
       "0           0  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
       "1           1  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
       "2           2  0   0   0  0    0    0     0    0    1  Mozilla            0   \n",
       "3           3  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
       "4           4  0   0   0  0    0    0     0    0    0  Mozilla            0   \n",
       "\n",
       "   label                                               text TER_targets  \\\n",
       "0      0             websites & communications terms of use         NaN   \n",
       "1      0  please read the terms of this entire document ...         NaN   \n",
       "2      1  by accessing or signing up to receive communic...         NaN   \n",
       "3      0  our websites include multiple domains such as ...         NaN   \n",
       "4      0  you may also recognize our websites by nicknam...         NaN   \n",
       "\n",
       "  LTD_targets A_targets CH_targets CR_targets  \n",
       "0         NaN       NaN        NaN        NaN  \n",
       "1         NaN       NaN        NaN        NaN  \n",
       "2         NaN       NaN        NaN        NaN  \n",
       "3         NaN       NaN        NaN        NaN  \n",
       "4         NaN       NaN        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we were trying binary classification, which wasn't producing great results, let's see if we can get better results using the individual types of labels as classified by the text. The logic here is that because each type of potential unfairness likely has some semantic differences, conglomerating them all into one made it difficult to pick them all out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0    0.893324\n",
       "6    0.029681\n",
       "2    0.016849\n",
       "8    0.015085\n",
       "9    0.011853\n",
       "3    0.010286\n",
       "4    0.006661\n",
       "5    0.006122\n",
       "1    0.005192\n",
       "7    0.004947\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'] = df.apply(lambda row: (1 if row['A'] == 1 else 2 if row['CH'] == 1 else 3 if row['CR'] == 1 else 4 if row['J'] == 1 else 5 if row['LAW'] == 1 else 6 if row['LTD'] == 1 else 7 if row['PINC'] == 1 else 8 if row['TER'] == 1 else 9 if row['USE'] == 1 else 0),axis=1)\n",
    "x_multi = df['text']\n",
    "y_multi = df['labels']\n",
    "df['labels'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'FAIR',\n",
       " 1: 'A',\n",
       " 2: 'CH',\n",
       " 3: 'CR',\n",
       " 4: 'J',\n",
       " 5: 'LAW',\n",
       " 6: 'LTD',\n",
       " 7: 'PINC',\n",
       " 8: 'TER',\n",
       " 9: 'USE'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\n",
    "    'FAIR':0,\n",
    "    'A':1,\n",
    "    'CH':2,\n",
    "    'CR':3,\n",
    "    'J':4,\n",
    "    'LAW':5,\n",
    "    'LTD':6,\n",
    "    'PINC':7,\n",
    "    'TER':8,\n",
    "    'USE':9\n",
    "}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Label Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df['text'], df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
    "rus = RandomUnderSampler(sampling_strategy=1)\n",
    "x_train_res, y_train_res = rus.fit_resample(pd.DataFrame(x_train), pd.DataFrame(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0        1730\n",
       "1        1730\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(x_train_res,y_train_res,left_index=True,right_index=True)\n",
    "test_df = pd.merge(x_test,y_test,left_index=True,right_index=True)\n",
    "val_df = pd.merge(x_val,y_val,left_index=True,right_index=True)\n",
    "train_df = train_df.sample(frac=1)\n",
    "subset_df = train_df.sample(50)\n",
    "test_subset_df = test_df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "    \"train\":Dataset.from_pandas(train_df),\n",
    "    \"test\":Dataset.from_pandas(test_df),\n",
    "    \"val\":Dataset.from_pandas(val_df),\n",
    "    \"train_subset\":Dataset.from_pandas(subset_df),\n",
    "    \"test_subset\":Dataset.from_pandas(test_subset_df)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], max_length=512,padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0112fdc0fa2f4f80ba5cd46ca5081015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfac324a867451a94b186065309d6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3782b991e55440e3b6136bd5cbac6d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffa42385c254d26bafde0030f90506f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a28073765e40f3bc24d3c89616aa27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dict = dataset_dict.map(tokenize, batched=True, batch_size=len(dataset_dict))\n",
    "tokenized_dict.set_format('torch', columns=['input_ids', 'attention_mask',\"token_type_ids\",'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct a custom classifier classifier to classify sentences as potentially unfair or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_v1(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(Classifier_v1,self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name,config = AutoConfig.from_pretrained(model_name,\n",
    "                                                                                              output_attention = True,\n",
    "                                                                                              output_hidden_state = True))\n",
    "        # New Layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(self.model.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input = None, attention_mask = None, token_type_ids = None,input_ids = None, labels = None):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        pooler_output = outputs.pooler_output[0]\n",
    "        output = self.hidden(pooler_output)\n",
    "        logit = self.sigmoid(output)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dict['train'], shuffle=True, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dict['test'], shuffle=True, batch_size=1,collate_fn=data_collator\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_dict['val'], shuffle=True, batch_size=1,collate_fn=data_collator\n",
    ")\n",
    "train_subset_dataloader = DataLoader(\n",
    "    tokenized_dict['train_subset'], shuffle=True, batch_size=1,collate_fn=data_collator\n",
    ")\n",
    "test_subset_dataloader = DataLoader(\n",
    "    tokenized_dict['test_subset'], shuffle=True, batch_size=1,collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_dataloader,test_dataloader,epochs,loss_fn,optimizer,num_train_samples,num_test_samples,lr_scheduler=None,testing=False):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_labels = []\n",
    "        train_preds = []\n",
    "        train_loss = 0\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            label = batch['labels'].to(torch.float32).to(device)\n",
    "            output = model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "            loss = loss_fn(output[0],label)\n",
    "            train_loss += loss.item()\n",
    "            pred = torch.round(output[0])\n",
    "            train_preds.append(pred.detach().cpu().numpy())\n",
    "            train_labels.append(label.detach().cpu().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step(loss)\n",
    "        print(f\"Train Epoch: {epoch + 1} | Accuracy: {accuracy_score(train_labels,train_preds)} | Precision: {precision_score(train_labels,train_preds)} | Recall: {recall_score(train_labels,train_preds)} | F1: {f1_score(train_labels,train_preds)} | Loss: {train_loss/num_train_samples}\")\n",
    "        if testing:\n",
    "            preds = []\n",
    "            labels = []\n",
    "            test_loss = 0\n",
    "            for batch in tqdm.tqdm(test_dataloader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                label = batch['labels'].to(torch.float32).to(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "                loss = loss_fn(output[0],label)\n",
    "                test_loss += (loss.item())\n",
    "                pred = torch.round(output[0])\n",
    "                preds.append(pred.detach().cpu().numpy())\n",
    "                labels.append(batch[\"labels\"].detach().cpu().numpy())\n",
    "            print(f\"Test Epoch: {epoch + 1} | Accuracy: {accuracy_score(labels,preds)} | Precision: {precision_score(labels,preds)} | Recall: {recall_score(labels,preds)} | F1: {f1_score(labels,preds)} | Loss: {test_loss/num_test_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_dataloader,loss_fn,num_samples):\n",
    "    model.eval()\n",
    "    raw_preds = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    test_loss = 0\n",
    "    for batch in tqdm.tqdm(test_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        label = batch['labels'].to(torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        loss = loss_fn(output[0],label)\n",
    "        test_loss += (loss.item())\n",
    "        raw_preds.append(output[0].detach().cpu().numpy())\n",
    "        pred = torch.round(output[0])\n",
    "        preds.append(pred.detach().cpu().numpy())\n",
    "        labels.append(batch[\"labels\"].detach().cpu().numpy())\n",
    "    print(f\"Accuracy: {accuracy_score(labels,preds)} | Precision: {precision_score(labels,preds)} | Recall: {recall_score(labels,preds)} | F1: {f1_score(labels,preds)} | Loss: {test_loss/num_samples}\")\n",
    "    return raw_preds,preds,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better than before, but there is still a lot of room for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not working, even with oversampling we are still not getting good results. I think the problem is that the pooler output, which is the (mean?) of all the 12 hidden states of BERT that we are using as input to our additional classifier is not capturing the information we need to understand the fairness of a sentence. Instead of using the pooler output, let's use a concatenation of all the hidden states of the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_V2(nn.Module):\n",
    "    '''\n",
    "    This model is similar to the first one but instead of using the pooler output, it uses the hidden states of the model\n",
    "    The 'hidden_states_used' parameter is used to determine how many hidden states to use, smaller values of this will be less computationally expensive, but likely less accurate\n",
    "    '''\n",
    "    def __init__(self, model_name ,num_labels,hidden_states_used):\n",
    "        super(Classifier_V2,self).__init__()\n",
    "        self.hidden_states_used = hidden_states_used\n",
    "        self.model = BertModel.from_pretrained(model_name,config = BertConfig.from_pretrained(model_name,output_hidden_states = True,num_labels=num_labels))\n",
    "        self.hidden1 = nn.Linear(self.model.config.hidden_size*self.model.config.max_position_embeddings*self.hidden_states_used, 64)\n",
    "        self.hidden_p = nn.Linear(self.model.config.hidden_size, 64)\n",
    "        self.fc = nn.Linear(64, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        if num_labels == 1:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "    \"\"\"\n",
    "    In the forward function we take the hidden states of the model and concatenate them along the first dimension, and then we pass them through a linear layer\n",
    "    We also take the pooler output of the sentence and pass it through a separate linear layer\n",
    "    We then add the two outputs together and pass them through a final linear layer to classify the output\n",
    "    We do this in hopes that the model will learn to use the hidden states to learn more about the semantic meaning of the text,\n",
    "    while the pooler output will learn more about the overall meaning of the text\n",
    "    \"\"\"\n",
    "    def forward(self, attention_mask = None, token_type_ids = None,input_ids = None):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        hidden_states = torch.cat(outputs.hidden_states[-self.hidden_states_used:],dim=0).view(1,-1)\n",
    "        pooler_output = outputs.pooler_output\n",
    "        x_pooler = self.hidden_p(self.dropout(pooler_output))\n",
    "        x_hidden = self.hidden1(self.dropout(hidden_states))\n",
    "        x = torch.add(x_pooler,x_hidden)\n",
    "        output = self.fc(x)\n",
    "        logit = self.activation(output)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_v5 = Classifier_V2('distilbert/distilbert-base-uncased',1,6).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = Adam(model_v5.parameters(),lr =.00000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v5.load_state_dict(torch.load('../models/distil_bert_6_.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3460/3460 [51:00<00:00,  1.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Accuracy: 0.7569364161849711 | Precision: 0.7515563101301641 | Recall: 0.7676300578034682 | F1: 0.7595081498427223 | Loss: 0.49785690285878476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2042/2042 [09:12<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 1 | Accuracy: 0.6077375122428991 | Precision: 0.20591233435270132 | Recall: 0.9017857142857143 | F1: 0.33526970954356844 | Loss: 0.6737446827778865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3460/3460 [44:02<00:00,  1.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 | Accuracy: 0.7638728323699422 | Precision: 0.7586402266288952 | Recall: 0.7739884393063584 | F1: 0.7662374821173105 | Loss: 0.5005654173642747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2042/2042 [08:52<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 2 | Accuracy: 0.8604309500489716 | Precision: 0.4043887147335423 | Recall: 0.5758928571428571 | F1: 0.47513812154696133 | Loss: 0.37948325079204204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3460/3460 [49:34<00:00,  1.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 | Accuracy: 0.7745664739884393 | Precision: 0.775842044134727 | Recall: 0.7722543352601156 | F1: 0.7740440324449595 | Loss: 0.490143885533359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2042/2042 [09:20<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 3 | Accuracy: 0.885896180215475 | Precision: 0.4782608695652174 | Recall: 0.4419642857142857 | F1: 0.4593967517401392 | Loss: 0.32341755513197823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3460/3460 [45:57<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 | Accuracy: 0.7679190751445086 | Precision: 0.7665324899367453 | Recall: 0.7705202312138728 | F1: 0.7685211876621505 | Loss: 0.49219198697081745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2042/2042 [10:30<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 4 | Accuracy: 0.5230166503428012 | Precision: 0.18003412969283278 | Recall: 0.9419642857142857 | F1: 0.3022922636103152 | Loss: 0.7917235655314221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3460/3460 [43:56<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 | Accuracy: 0.7684971098265896 | Precision: 0.7619853355893965 | Recall: 0.7809248554913295 | F1: 0.7713388524122181 | Loss: 0.4864844889694292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2042/2042 [09:41<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 5 | Accuracy: 0.7830558276199804 | Precision: 0.30127041742286753 | Recall: 0.7410714285714286 | F1: 0.42838709677419357 | Loss: 0.4726684592494579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3460/3460 [42:57<00:00,  1.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 | Accuracy: 0.7809248554913295 | Precision: 0.7786697247706422 | Recall: 0.784971098265896 | F1: 0.7818077144502015 | Loss: 0.46818796901477805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2042/2042 [09:47<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 6 | Accuracy: 0.9020568070519099 | Precision: 0.6132075471698113 | Recall: 0.29017857142857145 | F1: 0.3939393939393939 | Loss: 0.279273193060911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model_v5,train_dataloader,test_dataloader,6,loss_fn,optimizer,num_train_samples=3460,num_test_samples=2042,testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2042/2042 [13:44<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.910871694417238 | Precision: 0.7674418604651163 | Recall: 0.2894736842105263 | F1: 0.42038216560509556 | Loss: 0.2507265433188207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = test(model_v5,val_dataloader,loss_fn,2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_v5.state_dict(),'../models/distil_bert_6_.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del loss_fn\n",
    "# del optimizer\n",
    "# del model_v2\n",
    "#del scheduler\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
